\documentclass[12pt, letterpaper]{article}
\usepackage{ifpdf}
\usepackage{mla}

\newcommand{\comment}[1]{}

\begin{document}
\begin{mla}
	{D.}{Choi}
	{Dr. Schroeder}
	{Writing 150, Section 64675}
	{28 September 2020}
	{Teaching Bias}


\comment{
The technology of the modern day is brilliant and terrifying. The tools
available to humanity now can have consequences on a previously unimaginable
scale\textemdash{we} can power the world with electricity, and in the process
destroy the ecosphere. We can inquire into the particle nature of the
universe, and use that knowledge to create mass-murder weapons. We can try to
teach artificial intelligence to detect and predict criminal activity, and
inadvertently establish an impenetrable surveillance state.
}

We already live in a surveillance state. Mail records, wiretapping, security
cameras, the smartphones in our pockets\textemdash{these} are all tools used
today to facilitate mass surveillance in the United States, but the sea of
data is vast and incomprehensible to any individual human. The advancement of
machine learning technology is supposed to be the perfect answer for this
problem of scale, allowing black-boxed algorithms to work the data on our
behalf, but\ldots\ do we really want this to be our solution? Can we trust
the artificial intelligence systems of today to generate unbiased conclusions,
and can we trust ourselves to handle the systems we develop in an ethical
and judicious manner?

To tackle these questions, let us consider the case of
Robert Julian-Borchak Williams's wrongful arrest by the Detroit Police
Department. Williams was arrested in January 2020 because the Michigan State
Police's facial recognition system identified him for a shoplifting incident
in October 2018. Once Detroit police interrogated Williams for the crime
(after detaining him), it became immediately clear to the detectives that the
system identified the wrong face\textemdash{although} the shoplifter in the
security camera footage and Williams are both African American, they are
obviously different people.

To understand why this incident occurred, why the facial recognition was
flawed, and why the Detroit Police Department put so much blind trust into a
completely nontransparent system, the incident's overall theme of implicit
racial bias must be addressed. Peggy McIntosh's concept of white privilege is
useful for this kind of analysis, and by approaching Williams's arrest through
the framework of white privilege, we can first see that the case is a
reflection of our society's current inability to identify systemic factors of
racism, and thus refute the claim that the implementation of
machine-learning-based surveillance systems is appropriate for the modern day.

To begin, let us discover and enumerate where white privilege played a role in
the arrest of Robert Julian-Borchak Williams. The common ignorance for
systemic factors of racism described by white privilege occurred in three
unique places in this case: Firstly in the flawed development of the Michigan
State Police's facial recognition system that caused it to be racially biased,
secondly in the institution of that very system when it was known that such a
system was at risk to be biased, and thirdly in the Detroit Police
Department's blind trust for that system.

White privilege prevented the Michigan State Police's facial recognition
system from being robust for all races. At least since the publication of ``An
other-race effect for face recognition algorithm'' in 2011, it has been known
that facial recognition algorithms can be biased toward one race over another.
By comparing the performance of algorithms developed in Western countries by
those developed in East Asian countries, the paper was able to conclude that
``state-of-the-art face recognition algorithms, like humans, struggle with
`other-race face' recognition''. More recent studies continue to confirm this
understanding, such as the ones cited in the 2016 article ``Facial-Recognition
Software Might Have a Racial Bias Problem''. This racial bias may exist in
modern machine-learning-based algorithms because datasets used to train these
algorithms often do not adequately represent racial minorities. DataWorks
Plus, the Michigan State Police's supplier for facial recognition technology,
does not acknowledge any of this information: DataWorks Plus does ``no
scientific or formal testing for accuracy or bias'', does not publicly
disclose their training datasets, and provides no factual basis for their
marketing that their technology is accurate and reliable. The fact that racial
minorities are systemically disregarded when obtaining training data and also
that DataWorks Plus does seem to care about this issue can both be
attributed to white privilege. In ``White Privilege: Unpacking the Invisible
Knapsack'', Peggy McIntosh argues that white people are ``taught to think of
their lives as morally neutral, normative, and average, and also ideal''. By
considering whiteness as the default, researchers and engineers privilege
white subjects for their development of facial recognition systems, and
thereby disadvantage persons of color.

By taking all of that into consideration, the state of Michigan should have
understood that continuing to authorize the use of DataWorks Plus's facial
recognition technology for police use would further facilitate systemic
racism. From the abundance of research there is about the prevalence of racial
bias in facial recognition, the state of Michigan must have either not
investigated the potential danger in using DataWorks Plus's software, or
willfully chosen to ignore the potential danger and implement the system
anyway. For either case, what could explain the Michigan government's blind
irresponsibility? According to McIntosh, the dominant group teaches its
members (like herself) to not see embedded (systemic) forms of oppression like
white privilege: ``I was taught to recognize racism only in individual acts of
meanness by members of my group, never in invisible systems conferring
unsought racial dominance on my group from birth.'' This can make the dominant
group vulnerable to accepting potentially oppressive systems because ``to
redesign social systems, we need first to acknowledge their colossal unseen
dimensions. The silences and denials surrounding privilege are the key
political tool here.'' In summary, systems of oppressive power seek to
maintain their dominance in society by silencing discussion about their
mechanisms. From this, it can be inferred that Michigan failed to recognize
that they would institute a system that prioritizes the innocence of white
people over others, because it is already affected by its own internal systems
of racism. Oppression reinforces oppression.

The blinding nature of white privilege also played a role in how the members
of the Detroit Police Department trusted their facial recognition system. The
Detroit Police arrested Robert Julian-Borchak Williams after their system
falsely identified him without preparing any additional probable cause. This
is stark opposition to Michigan's acceptable use policy for their Statewide
Network of Agency Photos: ``[Facial Recognition] is not a form of positive
identification and results shall be considered an investigative lead only''.
Furthermore, the system itself has a terrible track record on black subjects.
The system has been used almost exclusively on black people in 2020, and
according to Detroit Police Chief James Craig, ``If we were just to use the
technology by itself, to identify someone, I would say 96 percent of the time
it would misidentify''. The Detroit Police Department's trust in their facial
recognition system's ability to correctly identify black people is irrational
at best, supremacist at worst. McIntosh writes that this ``obliviousness about
white advantage'' (and equivalently, obliviousness about black disadvantage)
is a core aspect of white privilege.

\vspace{\baselineskip}
\noindent
Conclusions:
\begin{itemize}
	\item On a technological, institutional, and individual level, our society
	is not yet properly able to assess systemic forms of racism.
	\item Because of this, nontransparent systems like facial recognition
	should not be implemented for police or surveillance related use.
	\item Critical race theory concepts like white privilege allows us to
	examine where we can improve.
	\item Use miscellaneous lessons as examples?
\end{itemize}


% Sources:
% https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html
% https://nationalseedproject.org/Key-SEED-Texts/white-privilege-unpacking-the-invisible-knapsack
% https://dl.acm.org/doi/10.1145/1870076.1870082
% https://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/
% https://medium.com/@Khullani/facial-recognition-software-2a6d7048c062
% https://raconteur.net/technology/biometrics-ethics-bias
% https://www.brown.senate.gov/newsroom/press/release/brown-blasts-dataworks-unreliable-facial-recognition-technology
% https://www.michigan.gov/msp/0,4643,7-123-72297_64747_64749-357133--,00.html
% https://www.vice.com/en_us/article/dyzykz/detroit-police-chief-facial-recognition-software-misidentifies-96-of-the-time



\end{mla}
\end{document}
