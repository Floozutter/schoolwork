\documentclass[12pt, letterpaper]{article}
\usepackage{ifpdf}
\usepackage{mla}

\newcommand{\comment}[1]{}

\begin{document}
\begin{mla}
	{D.}{Choi}
	{Dr. Schroeder}
	{Writing 150, Section 64675}
	{28 September 2020}
	{Teaching Bias}


\comment{
The technology of the modern day is brilliant and terrifying. The tools
available to humanity now can have consequences on a previously unimaginable
scale\textemdash{we} can power the world with electricity, and in the process
destroy the ecosphere. We can inquire into the particle nature of the
universe, and use that knowledge to create mass-murder weapons. We can try to
teach artificial intelligence to detect and predict criminal activity, and
inadvertently establish an impenetrable surveillance state.
}

We already live in a surveillance state. Mail records, wiretapping, security
cameras, the smartphones in our pockets\textemdash{these} are all tools used
today to facilitate mass surveillance in the United States, but the sea of
data is vast and incomprehensible to any individual human. The advancement of
machine learning technology is supposed to be the perfect answer for this
problem of scale, allowing black-boxed algorithms to work the data on our
behalf, but\ldots\ do we really want this to be our solution? Can we trust
the artifical intelligence systems of today to generate unbiased conclusions,
and can we trust ourselves to handle the systems we develop in an ethical
and judicious manner?

To tackle these questions, let us consider the case of
Robert Julian-Borchak Williams's wrongful arrest by the Detroit Police
Department. Williams was arrested in January 2020 because the Michigan State
Police's facial recognition system identified him for a shoplifting incident
in October 2018. Once Detroit police interrogated Williams for the crime
(after detaining him), it became immediately clear to the detectives that the
system identified the wrong face\textemdash{although} the shoplifter in the
security camera footage and Williams are both African American, they are
obviously different people.

To understand why this incident occurred, why the facial recognition was
flawed, and why the Detroit Police Department put so much blind trust into a
completely nontransparent system, the incident's overall theme of implicit
racial bias must be addressed. Peggy McIntosh's concept of white privilege is
useful for this kind of analysis, and by approaching Williams's arrest through
the framework of white privilege, we can first see that the case is a
reflection of our society's current inability to identify systemic factors of
racism, and thus refute the claim that the implementation of
machine-learning-based surveillance systems is appropriate for the modern day.

To begin, let us discover and enumerate where white privilege played a role in
the arrest of Robert Julian-Borchak Williams. The common ignorance for
systemic factors of racism described by white privilege occurred in three
unique places in this case: Firstly in the flawed development of the Michigan
State Police's facial recognition system that caused it to be racially biased,
secondly in the institution of that very system when it was known that such a
system was at risk to be biased, and thirdly in the Detroit Police
Department's blind trust for that system.

White privilege prevented the Michigan State Police's facial recognition
system from being robust for all races. At least since the publication of ``An
other-race effect for face recognition algorithm'' in 2011, it has been known
that facial recognition algorithms can be biased toward one race over another.
By comparing the performance of algorithms developed in Western countries by
those developed in East Asian countries, the paper was able to conclude that
``state-of-the-art face recognition algorithms, like humans, struggle with
`other-race face' recognition''. More recent studies continue to confirm this
understanding, such as the ones cited in the 2016 article ``Facial-Recognition
Software Might Have a Racial Bias Problem''. This racial bias may exist in
modern machine-learning-based algorithms because datasets used to train these
algorithms often do not adequately represent ethnic minorities. DataWorks
Plus, the Michigan State Police's supplier for facial recognition technology,
does not seem to consider any of this information. DataWorks Plus does ``no
scientific or formal testing for accuracy or bias'', does not publicly
disclose their training datasets, and provides no factual basis for their
marketing that their technology is accurate and reliable. To completely
disregard all of this is an unacceptable form of ignorance on the part of
DataWorks Plus, the exact same kind of ignorance described by Peggy McIntosh
that denies the fact that systems of dominance exists.

[Unfinished.]

% Sources:
% https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html
% https://nationalseedproject.org/Key-SEED-Texts/white-privilege-unpacking-the-invisible-knapsack
% https://dl.acm.org/doi/10.1145/1870076.1870082
% https://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/
% https://medium.com/@Khullani/facial-recognition-software-2a6d7048c062
% https://raconteur.net/technology/biometrics-ethics-bias
% https://www.brown.senate.gov/newsroom/press/release/brown-blasts-dataworks-unreliable-facial-recognition-technology


\end{mla}
\end{document}
