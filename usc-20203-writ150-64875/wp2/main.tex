\documentclass[12pt, letterpaper]{article}
\usepackage{ifpdf}
\usepackage{mla}

\newcommand{\comment}[1]{}

\begin{document}
\begin{mla}
	{D.}{Choi}
	{Dr. Schroeder}
	{Writing 150, Section 64675}
	{1 October 2020}
	{Biased Facial Recognition: Racism in Scope}


We already live in a surveillance state. Mail records, wiretapping, security
cameras, the smartphones in our pockets\textemdash{these} are all tools used
today to facilitate mass surveillance in the United States, but the sea of
data is vast and incomprehensible to any individual human. The advancement of
machine learning technology is supposed to be the perfect answer for this
problem of scale, allowing black-boxed algorithms to work the data on our
behalf, but\ldots\ do we really want this to be our solution? Can we trust
the artificial intelligence systems of today to generate unbiased conclusions,
and can we trust ourselves to handle the systems we develop in an ethical
and judicious manner?

To tackle these questions, let us consider the case of
Robert Julian-Borchak Williams's wrongful arrest by the Detroit Police
Department. Williams was arrested in January 2020 because the Michigan State
Police's facial recognition system identified him for a shoplifting incident
in October 2018 (Hill). Once Detroit police interrogated Williams for the crime
(after detaining him), it became immediately clear to the detectives that the
system identified the wrong face\textemdash{although} the shoplifter in the
security camera footage and Williams are both African American, they are
obviously different people.

To understand why this incident occurred, why the facial recognition was
flawed, and why the Detroit Police Department put so much blind trust into a
completely nontransparent system, the incident's overall theme of implicit
racial bias must be addressed. Peggy McIntosh's concept of white privilege is
useful for this kind of analysis: By approaching Williams's arrest through the
framework of white privilege, we can first see that the case is a reflection
of our society's current inability to identify systemic factors of racism, and
thus refute the claim that the implementation of machine-learning-based
surveillance systems is appropriate for the modern day.

To begin, let us discover and enumerate where white privilege played a role in
the arrest of Robert Julian-Borchak Williams. The common ignorance for
systemic factors of racism described by white privilege occurred in three
unique scopes in this case: Firstly in the flawed development of the Michigan
State Police's facial recognition system that caused it to be racially biased,
secondly in the institution of that very system when it was known that such a
system was at risk to be biased, and thirdly in the Detroit Police
Department's blind trust for that system.

White privilege prevented the Michigan State Police's facial recognition
system from being robust for all races. At least since the publication of ``An
other-race effect for face recognition algorithm'' in 2011, it has been known
that facial recognition algorithms can be biased toward one race over another
(Phillips et al). By comparing the performance of algorithms developed in
Western countries by those developed in East Asian countries, the paper was
able to conclude that ``state-of-the-art face recognition algorithms, like
humans, struggle with `other-race face' recognition''. More recent studies
continue to confirm this understanding, such as the ones cited in the 2016
article ``Facial-Recognition Software Might Have a Racial Bias Problem''
(Garvie and Frankle). This racial bias may exist in modern
machine-learning-based algorithms because datasets used to train these
algorithms often do not adequately represent racial minorities
(Abdullahi, Yeung). DataWorks Plus, the Michigan State Police's supplier for
facial recognition technology, does not acknowledge any of this information:
DataWorks Plus does ``no scientific or formal testing for accuracy or bias'',
does not publicly disclose their training datasets, and provides no factual
basis for their marketing that their technology is accurate and reliable
(Brown). The fact that racial minorities are systemically disregarded when
obtaining training data and also that DataWorks Plus does seem to care about
this issue can both be attributed to white privilege. In ``White Privilege:
Unpacking the Invisible Knapsack'', Peggy McIntosh argues that white people
are ``taught to think of their lives as morally neutral, normative, and
average, and also ideal''. By considering whiteness as the default,
researchers and engineers privilege white subjects for their development of
facial recognition systems, and thereby disadvantage persons of color.

By taking all of that into consideration, the state of Michigan should have
understood that continuing to authorize the use of DataWorks Plus's facial
recognition technology for police use would further facilitate systemic
racism. From the abundance of research there is about the prevalence of racial
bias in facial recognition, the state of Michigan must have either not
investigated the potential danger in using DataWorks Plus's software, or
willfully chosen to ignore the potential danger and implement the system
anyway. For either case, what could explain the Michigan government's blind
irresponsibility? According to McIntosh, the dominant group teaches its
members, like herself, to not see embedded (systemic) forms of oppression like
white privilege: ``I was taught to recognize racism only in individual acts of
meanness by members of my group, never in invisible systems conferring
unsought racial dominance on my group from birth.'' This can make the dominant
group vulnerable to accepting potentially oppressive systems because ``to
redesign social systems, we need first to acknowledge their colossal unseen
dimensions. The silences and denials surrounding privilege are the key
political tool here.'' In other words, systems of oppressive power seek to
maintain their dominance by socially disincentivizing the discussion of their
mechanisms. From this, it can be inferred that Michigan failed to recognize
that they would institute a system that prioritizes the innocence of white
people over others, because it is already affected by its own systemic racism
internally.

This self-obscuring nature of white privilege also played a role in how the
members of the Detroit Police Department trusted their facial recognition
system. The Detroit Police arrested Robert Julian-Borchak Williams after their
system falsely identified him without preparing any additional probable cause.
This is in stark opposition to Michigan's acceptable use policy for their
Statewide Network of Agency Photos: ``[Facial recognition] is not a form of
positive identification and results shall be considered an investigative lead
only'' (Michigan). Furthermore, the system itself has a terrible track record
on black subjects (Koebler). The system has been used almost exclusively on
black people in 2020, and according to Detroit Police Chief James Craig, ``If
we were just to use the technology by itself, to identify someone, I would say
96 percent of the time it would misidentify''. All of this indicates that the
Detroit Police Department's trust in their facial recognition system's ability
to correctly identify minorities is irrational at best, supremacist at worst.
McIntosh identifies this kind of thinking on the individual level as
``obliviousness about white advantage'', and describes it as another mechanism
of white privilege that helps to maintain itself: ``Keeping most people
unaware that freedom of confident action is there for just a small number of
people props up those in power and serves to keep power in the hands of the
same groups that have most of it already''. The obliviousness caused by white
privilege caused the Detroit police to further reinforce the status quo of
systemic racial oppression unquestioningly.

In summary, viewing the case of Robert Julian-Borchak Williams's arrest
through the lens of white privilege shows that on a technological,
institutional, and individual level, our modern-day society is still
vulnerable to the effects of systemic racism. The development of DataWorks
Plus's facial recognition software was impaired because of white normativity.
The implementation of DataWorks Plus's software for the Michigan State Police
was allowed because dominant groups tend to accept establishing potential
causes of embedded oppression. And the members of the Detroit police blindly
trusted their facial recognition because privilege engenders an obliviousness
toward systemic forms of advantage and disadvantage.

Using this information, we can now try to answer the motivating questions we
opened with. Can we trust the artificial intelligence systems of today to
generate unbiased conclusions? No, although an algorithm's judgement may seem
cold and impartial, its design is ultimately human, and human designers may
inadvertently impart their biases onto it as we saw with DataWorks Plus. Can
we trust ourselves to handle the systems we develop in an ethical and
judicious manner? No, as we saw with the state of Michigan and the Detroit
police, we can be entirely too trusting of systems when it is not immediately
clear that they are biased. Because modern artificial intelligence systems
lack transparency, we can not accurately assess them on an institutional or
individual level. Overall, we are not yet ready to handle the responsibility
that AI technology demands of us.

To move forward, continuing the discussion and education of critical theory
concepts like white privilege can help defend ourselves from establishing
structures that further systemic oppression. For researchers and engineers,
understanding how their bias may be reflected in the technology they create
can help them develop more egalitarian and human-centric tools for society.
For political bodies, understanding where systemic oppression can arise can
help them prevent the institution of harmful structures. Finally, for
individuals, understanding how we may be influenced by oppressive systems can
help us break free from them in our day-to-day decision-making. To challenge
the systems of oppression we have created for ourselves, we must first learn
to see them.


\begin{workscited}
	\bibent Abdullahi, Khullani M. ``Facial Recognition Software: Why Canâ€™t it
	Accurately Identify Black Faces?''. \textit{Medium}, October 23, 2016.

	\bibent Brown, Sherrod. ``Brown Blasts DataWorks Plus' Unreliable Facial
	Recognition Technology''. \textit{www.brown.senate.gov}, July 1, 2020.

	\bibent Garvie, Clare and Jonathan Frankle. ``Facial-Recognition Software
	Might Have a Racial Bias Problem''. \textit{The Atlantic}, April 7, 2016.

	\bibent Hill, Kashmir. ``Wrongfully Accused by an Algorithm''.
	\textit{The New York Times}, June 24, 2020.
	
	\bibent Koebler, Jason. ``Detroit Police Chief: Facial Recognition
	Software Misidentifies 96\% of the Time''. \textit{Vice}, June 29, 2020.
	
	\bibent McIntosh, Peggy. ``White Privilege: Unpacking the Invisible
	Knapsack''. \textit{Peace and Freedom Magazine}, July, 1989.
	
	\bibent Michigan. ``SNAP Acceptable Use Policy''. \textit{Michigan State
	Police}, March 7, 2016.
	
	\bibent Phillips, P. Jonathon, et al. ``An other-race effect for face
	recognition algorithms''. \textit{ACM Transactions on Applied Perception},
	February, 2011.
	
	\bibent Yeung, Peter. ``Why racial bias is still inherent in biometric
	tech''. \textit{Raconteur}, May 28, 2020.
\end{workscited}


% Sources:
% https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html
% https://nationalseedproject.org/Key-SEED-Texts/white-privilege-unpacking-the-invisible-knapsack
% https://dl.acm.org/doi/10.1145/1870076.1870082
% https://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/
% https://medium.com/@Khullani/facial-recognition-software-2a6d7048c062
% https://raconteur.net/technology/biometrics-ethics-bias
% https://www.brown.senate.gov/newsroom/press/release/brown-blasts-dataworks-unreliable-facial-recognition-technology
% https://www.michigan.gov/msp/0,4643,7-123-72297_64747_64749-357133--,00.html
% https://www.vice.com/en_us/article/dyzykz/detroit-police-chief-facial-recognition-software-misidentifies-96-of-the-time


\end{mla}
\end{document}
